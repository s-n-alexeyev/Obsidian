2024-01-26

Олег Вознесенский

[Оригинальная статья 1](https://habr.com/ru/companies/gazprombank/articles/788978/)
[Оригинальная статья 2](https://habr.com/ru/companies/gazprombank/articles/789404/)
```table-of-contents
title: Содержание
style: nestedList # TOC style (nestedList|inlineFirstLevel)
minLevel: 0 # Include headings from the specified level
maxLevel: 0 # Include headings up to the specified level
includeLinks: true # Make headings clickable
debugInConsole: false # Print debug info in Obsidian console
```
---

Гайд для собеседований по Kubernetes: главные (и не только) вопросы и как на них ответить
# Как проходят собеседования по Kubernetes?

Вопросы по Kubernetes достаточно часты на собеседованиях на инженерные вакансии, связанные с администрированием и эксплуатацией. Вопросы могут варьироваться от базовых, рассчитанных на механическую проверку теоретических знаний («объясните, что такое service»), до более сложных и комплексных, требующих глубинного понимания внутренних принципов Kubernetes и работы (каким образом опубликовать приложение, развёрнутое в Kubernetes). Давайте пойдём от базы в направлении возрастания сложности.

Собеседования по Kubernetes (k8s) могут иметь различную специфику с акцентом на администрирование и эксплуатацию этой системы. В зависимости от этого, а также от персонального опыта интервьюера, могут задаваться и те вопросы, на которые кандидат не знает ответа.

В такой ситуации хорошим вариантом ответа может быть: «К сожалению, мне не приходилось сталкиваться с подобными кейсами на практике. Но я могу достаточно быстро найти ответ на этот вопрос при помощи поискового сервиса или нейронки, если возникнет такая необходимость».

И это нормальный ответ. IT-индустрия достаточно большая, всё в одну голову не вмещается.

# Какие вопросы задают на собеседованиях?

## Что такое Kubernetes?

На самом деле не самый простой и очевидный вопрос. Самый распространённый ответ — «система оркестрации контейнеров». На самом деле контейнер — слишком низкий уровень абстракции для k8s (он управляет POD’адми, на каждый POD создаётся минимум два контейнера). На мой взгляд, вариант, который более точно характеризует эту систему, скорее такой:

«Наиболее популярная на данный момент инфраструктурная платформа, содержащая в себе фреймворк для декларативного управления конфигурациями приложений на основе контейнеризации и (реже) виртуализации, которая в настоящее время является стандартом индустрии».

## Что такое POD?

Группа контейнеров, объединённых общей сетью (общий localhost, общий внешний IP). Это может быть нужно для запуска созависимых сервисов, которые работают совместно по сети (например, связки nginx + php-fpm) или используют общие файловые ресурсы (например, продюсер и сборщик логов).

## Для чего при старте PODа создаётся контейнер с процессом pause?

Для PODа создаются как минимум два контейнера. Первый контейнер с pause служит для того, чтобы обеспечить общую сеть (для него создаётся network namespace в linux — то есть все контейнеры PODа должны располагаться на одной ноде).

## Каким образом мы можем запустить в Kubernetes приложение (варианты: рабочую нагрузку, workload)?

Мы можем запускать PODы поодиночке, но, как правило, для этого используются более функциональные сущности (workloads [https://kubernetes.io/docs/concepts/workloads/](https://kubernetes.io/docs/concepts/workloads/)):

- Deployment — служит для развёртывания нужного количества PODов на основе единого шаблона.
- StatefulSet — похож на Deployment, но оптимизирован под работу с stateful-приложениями.
- DaemonSet — позволяет развернуть один экземпляр приложения на каждом доступном узле (применяется, как правило, для нужд, связанных с обслуживанием узлов).
- Job — способ запустить конечный процесс в контейнере и дождаться его успешного выполнения (описав при этом политики запуска вроде длительности и количества рестартов в случае падения).
- CronJob — то же, что и Job, но с запуском по расписанию. При этом ведётся журнал успешности запусков.

Есть ещё ReplicationController, но он считается устаревшим и практически не используется. На смену ему пришёл Deployment.

## Что такое ReplicaSet?

ReplicaSet — это сущность, которой управляет Deployment. Deployment создаёт новый ReplicaSet каждый раз, когда в его конфигурации происходят изменения, после чего происходит завершение PODов старого ReplicaSet и запуск PODов нового. При помощи ReplicaSet мы можем (в том числе) делать откат на предыдущие версии Deployment.

## Что такое Kubernetes probes?

Это проверки, которые осуществляются в течение жизненного цикла PODа. Они описываются для каждого контейнера PODа. Существуют три вида проверок.

- Startup probe — запускается сразу после старта PODа и применяется для приложений, которые имеют длительную процедуру инициализации. Пока она не завершена, другие пробы не запускаются.
- Readiness probe — проверка готовности PODа обрабатывать трафик (POD не добавляется в маршрутизацию трафика в service, если эта проверка не пройдена).
- Liveness probe — проверяет, функционирует ли приложение (в случае, если проверка не завершилась успехом, процесс в контейнере PODа перезапускается).

Readiness и liveness — независимые и запускаются после прохождения startup probe.

Существуют exec-, http-, tcp- и gprc-пробы. Проверки осуществляются сервисом kubelet на ноде, где запущен целевой POD.

## Какие существуют хорошие практики для создания проб?

Пробы должны проверять функционал приложения, задействованный в обработке запросов пользователя. Например, если в приложении есть панель администратора и веб-сайт, то проверять нужно ответ от сайта, причём не какой-то синтетический location, отдающий простой код ответа, но запуск простой функции, которая должна использовать механизмы приложения, близкие к обработке реальных пользовательских запросов.

Пробы — это дополнительная нагрузка на инфраструктуру, поэтому они не должны осуществлять тяжёлые, ресурсоёмкие запросы к приложению, равно как и не должны запускаться слишком часто. По возможности пробы также не должны проверять функционал, зависящий от внешних сервисов, — иначе можно получить каскадный сбой. Нужно быть очень осторожным с liveness-пробами, — они перезапускают процесс в контейнере и могут вызвать дополнительные проблемы, когда, например, на приложение пришла высокая нагрузка, оно занято обработкой пользовательских запросов и не может ответить на liveness-пробу (лучше сделать выделенный пул воркеров на прохождение liveness-пробы, если проверяемый сервис это позволяет). Лучше всего создавать сквозные проверки для readiness-пробы — например, если в PODе связка nginx + php-fpm, проверять можно только location nginx, который проксирует php-fpm (одним запросом проверяются оба сервиса).

## Что такое Pod Disruption Budget (PDB)?

Это функционал k8s, позволяющий держать запущенными минимально необходимое количество PODов приложения. То есть при возникновении события вроде evict’а PODов с ноды, drain`а ноды, удаления PODов и прочего k8s не удалит PODы, если общее количество PODов приложения ниже PDB.

## Что такое priority classes?

Это механизм k8s, позволяющий указать важность PODов для наших процессов. Например, если в одном кластере k8s у нас продуктовый и тестовые окружения, мы можем повысить priority class для PODов продуктового, и если, например, возникнет недостаток ресурсов на ноде, первыми для evict’а будут выбраны PODы тестовых окружений.

## Что такое POD eviction?

Это механизм, позволяющий освободить ноду от лишних PODов. Бывают evict’ы по ресурсам (когда на ноде не хватает памяти, места на диске или количества PID для процессов) и посредством вызова API — когда мы запускаем kubectl drain node.

## Каким образом мы можем вывести ноду из работы для обслуживания?

Мы можем запретить запуск PODов на ноде с помощью kubectl cordon и удалить PODы с ноды при помощи kubectl drain.

## Каким образом мы можем управлять размещением PODов на конкретных нодах кластера k8s?

Для этого существует несколько возможностей.

- NodeSelector/node affinity — механизм, позволяющий запускать PODы на нодах с определённым набором меток (labels). Это может быть полезно, если, например, PODы требуют определённого оборудования — скажем, у нас есть пул нод с GPU для нужд машинного обучения.
- Tains/tolerations — механизм, позволяющий запрещать запуск на ноде PODов (taint — описывается на ноде), которые не имеют разрешения (toleration — описывается на POD’е). Это может быть полезно, если у нас в кластере несколько окружений — мы можем выделить ноды под production и при помощи taint запретить запускать там PODы тестовых окружений.
- Pod affinity/antiAffinity — механизм, позволяющий группировать PODы разных приложений на общих нодах (если, например, им важен быстрый сетевой доступ) или, наоборот — заставлять их запускаться на разных нодах (например, чтобы распределить PODы одного приложения по нодам кластера для повышения отказоустойчивости в случае сбоя на ноде).

## Каким образом мы можем управлять вычислительными ресурсами в k8s?

Для управления ресурсами в кластере k8s служат resources requests / limits. Их можно выставить для CPU, memory и в последних версиях k8s — для GPU.

Requests применяются для того, чтобы определить, сколько ресурсов обычно потребляет наше приложение — на основании этих данных Kubernetes scheduler будет осуществлять выбор нод для запуска PODов (сумма request’ов всех контейнеров всех PODов должна быть меньше доступных на ноде ресурсов).

Limits применяются как предохранительный механизм, ограничивающий потребление ресурсов конкретным контейнером PODа. В случае превышения лимита процессорного времени будет применяться механизм thermal throttling, в случае превышения лимита памяти, — механизм OOM.

Модель выделения ресурсов, при которой requests меньше limits, называют burstable QoS, requests равно limits — guaranteed QoS.

Кроме того, мы можем выставить квоты ресурсов (resource quotas) на namespace (квоты по CPU, памяти, количеству запущенных PODов и размеру диска заказанных persistent volume), а также описать требования к resources requests, которые должны быть описаны на POD’ах в namespace при помощи limit ranges.

Также ресурсами приложений можно управлять при помощи автоскейлеров. В k8s из коробки доступны:

- HPA — horizontal pod autoscaler, который может увеличивать или уменьшать количество PODов приложения в зависимости от потребления ими CPU и/или памяти;
- VPA — vertical pod autoscaler, который может управлять resources requests / limits (есть разные режимы работы — от рекомендательного до принудительного изменения конфигурации ресурсов и перезапуска PODов с новыми настройками).

Ещё есть реализации автоскейлеров, которые при работе могут использовать какие-то внешние метрики (скажем, длину очереди), например, carpenter или KEDA. Если кластер работает в облачном окружении, мы можем использовать cluster autoscaler для заказа новых нод или удаления лишних, в зависимости от общей утилизации групп нод.

## Каким образом мы можем улучшить стабильность работы приложения в k8s?

Прежде всего мы должны описать probe для контейнеров в PODе, а также resources requests / limits. Далее лучше всего описать antiAffinity для PODов наших приложений, чтобы легче переживать сбои на конкретных нодах.

Если у нас в кластере работают как продуктовые, так и тестовые окружения, то хорошей практикой будет описать node selector и taints/tolerations, чтобы запускать production-приложения на выделенных нодах.

Если возможности выделить ноды под production нет или в рамках production мы можем выделить особо важные (core) сервисы, для них стоит поднять priority classes. Также есть смысл описать pod distruption budget для особо важных приложений. В случае с многопользовательской (multitenant) моделью использования кластера в неймспейсах пользователей стоит описывать resourceQuotas и limitRanges.

## Что такое Kubernetes service?

Это объект, который выполняет несколько важных функций.

- Разрешение DNS-имён: сервис предоставляет символьное DNS-имя, формирующееся на основе имени сервиса, неймспейса приложения и DNS-суффикса кластера для единообразного доступа к приложениям. Плюс сервисы с типом externalName могут описывать произвольные A-записи во внутреннем DNS кластера.
- Маршрутизация и балансировка трафика: сервис может предоставлять IP-адрес, при обращении к которому при помощи алгоритма round robin балансирует трафик на PODы приложения, либо выдавать IP-адреса PODов приложения на запрос к своему DNS-имени для организации балансировки на уровне приложения (режим headless с настройкой clusterIp: no).
- Service discovery: на основании описанных в нём меток service ищет PODы, которые соответствуют этим меткам и прошли readiness-пробу, и добавляет их в маршрутизацию трафика.
- Публикация приложений: мы можем использовать сервисы для того, чтобы предоставлять внешним пользователям доступ к приложениям в кластере.

## Каким образом мы можем предоставить приложение, которое работает в нашем кластере, k8s нашим пользователям?

— Если наше приложение работает по HTTP, мы можем использовать так называемый ingress-контроллер — реверс-прокси, интегрированный с Kubernetes API, который позволяет на основе описанных custom resources осуществлять маршрутизацию трафика по HTTP.

Можно использовать Kubernetes service для ряда задач.

- С типом externalIp — чтобы опубликовать порт сервиса на конкретных IP-адресах наших нод; с типом nodePort — чтобы опубликовать порт сервиса на всех нодах на произвольных портах в диапазоне от 30 000 до 64 535 (при этом есть интересная опция trafficPriority: local, позволяющая предпочитать PODы приложения, находящегося на локальной ноде при маршрутизации трафика).
- С типом loadBalancer — если наш k8s работает в облаке. При этом создаётся сервис, аналогичный nodePort, но вдобавок к нему заказывается load balancer, который осуществляет маршрутизацию на ноды кластера в порты, выбранные для nodePort.
- Можно использовать kubectl proxy, чтобы пробросить порт сервиса локально на нашу машину (может быть полезно для нужд разработки).
- Мы можем использовать nodeNetwork: true (проверить, когда появится интернет), чтобы разрешить приложению, работающему в контейнере, использовать сеть ноды и занимать сетевые порты непосредственно на интерфейсах ноды.

## Что такое Kubernetes ingress?

Это реверс-прокси, интегрированный с Kubernetes API, который позволяет на основе описанных в кластере специальных custom resources осуществлять доставку пользовательского трафика до приложений, развёрнутых в кластере. Существует множество реализаций этого паттерна. Например, ingress-nginx от создателей k8s на базе nginx или router в openshift на базе haproxy. Помимо доставки трафика, ингрессы позволяют создавать HTTPS-соединения на основе сертификатов, полученных, к примеру, от letsencrypt при помощи cert manager, а также делать многие другие вещи вроде timeouts/retry, limits, session affinity / sticky sessions, маршрутизации трафика для канареечных выкатов и т. п.

## Можем ли мы опубликовать приложение, работающее по бинарному протоколу, например postgresql, через ingress?

Да, многие ингресс-контроллеры поддерживают публикацию бинарных протоколов, но это неудобно. Kubernetes ingress сам, как правило, публикуется при помощи Kubernetes service. Для каждой публикации приложения по бинарному протоколу через ingress придётся дополнительно описывать Kubernetes service.

## Если нам всё равно нужно описывать Kubernetes service для публикации ingress, то зачем нам ingress?

Преимущество ingress-контроллера в том, что, опубликовав его единожды, мы получим возможность доставлять через него трафик всем нашим приложениям внутри кластера k8s, которые работают по HTTP на основе маршрутизации по URL/locations, HTTP headers и cookie. А также если появится такая необходимость, мы сможем использовать несколько ingress-контроллеров в кластере, разделяя их по ingress class.

## Допустим, у нас postgresql в кластере k8s, и разработчики просят к ней доступ. Каким образом мы можем решить этот вопрос?

Прежде всего, мы можем развернуть в кластере веб-основанный тулинг для работы с базой, например pgadmin, и опубликовать его через ingress для разработчиков.

Есть также вариант с использованием какого-нибудь инструмента для доступа разработчика к кластеру (например, Kubernetes dashboard или LENS) с возможностью сделать exec в POD и доступа к базе через утилиту командной строки.

Если разработчикам всё-таки нужен прямой сетевой доступ к базе (например, для использования своего любимого инструмента работы с БД), мы можем завести аккаунт для разработчиков в кластере и использовать kubectl proxy для публикации порта базы на localhost машины разработчика. Либо поднять в кластере сервер vpn.

На худой конец, есть возможность опубликовать базу через Kubernetes service или ingress, но в данном случае нам нужно позаботиться о защите соединения с БД (пользователи, доступы) и протокола (включить шифрование).

## Вы вводите в адресную строку браузера https://company.co. Приложение, работающее по этому адресу, запущено в Kubernetes. Опишите как можно подробнее, что произойдёт после нажатия Enter?

Сначала браузер попробует получить IP-адрес точки входа через DNS запрос (тут можно спросить интервьюера, стоит ли объяснять процесс разрешения DNS-имён).

Допустим, наш Kubernetes запущен в облаке как managed-решение. Скорее всего, там будет облачный load balancer, созданный сервисом с типом loadBalancer. Load balancer может быть application (ALB), и тогда он проксирует запрос, либо network (NLB), и тогда он маршрутизирует запрос.

Если это будет ALB, то, вероятно, на нём будет поддерживаться защищённая сессия (вопрос интервьюеру: стоит ли объяснять установку HTTPS-соединения?).

От Load balancer трафик попадёт на порт одной из нод кластера, который слушает связанный сервис.  Скорее всего для этого порта описаны правила iptables, при помощи которых запрос маршрутизируется в один из PODов ingress-контроллера.

Ingress-контроллер, согласно своей конфигурации, проксирует запрос в один из PODов сервиса, связанного с этим доменом и location. Он может маршрутизировать запрос согласно алгоритму round robin, либо применить какую-то дополнительные правила маршрутизации (например, session affinity или canary на основе весов или соединений, если это описано аннотациями на ингрессе).

Если POD приложения расположен на другой ноде, то пакет пройдёт процесс маршрутизации — нода с ингрессом согласно своей таблице маршрутизации выберет ноду PODа в качестве шлюза и перешлёт ей запрос от ингресса, нода доставит его в приложение в контейнере. Приложение обработает запрос и отошлёт ответ ингрессу, который проксирует это в Load Balancer, а тот, в свою очередь, вернёт его в браузер клиента.
## Каким образом организована сеть в k8s?  

В k8s существуют три типа сети: 
- node network — сеть, в которую объединены ноды; в зависимости от использования CNI-плагина, ноды могут работать только в одной подсети, либо в нескольких;
- pod network — сеть, в которой получают IP-адреса запускаемые PODы; 
- service network — сеть, в которой получают адреса Kubernetes services.Pod network и service network организуются при помощи так называемых CNI-плагинов.

Pod network и service network организуются при помощи так называемых CNI-плагинов.
## Что такое CNI-плагин и для чего он нужен?

Аббревиатура CNI расшифровывается как Container Network Interface. Он представляет собой некий уровень абстракции над реализацией сети. Мы можем работать с верхнеуровневыми абстракциями вроде «IP-адрес PODа», Endpoint. За то, как это будет реализовано на физическом уровне, отвечает CNI-плагин. Существует множество CNI-плагинов (например, Flannel, Calico, Cilium), которые реализуют разный функционал и показывают разную сетевую производительность. От простейших, которые используют для работы L3-маршрутизацию, правила iptables и IPVS, до достаточно сложных, которые, например, могут осуществлять шифрование внутреннего трафика в кластере, поддержку VLAN, маршрутизацию на основе IBGP, поддержку EGRESS и прочее.
## Что такое egress?  

Возможность назначить внешний IP-адрес для исходящего за пределы кластера k8s трафика приложений. Поддержка egress должна быть реализована на уровне CNI-плагина и может быть описана специальным объектом на уровне неймспейса.
## Как мы можем ограничить трафик в Kubernetes?  

Для ограничения трафика от приложений используется объект networkpolicy. При помощи него мы можем ограничивать входящий и исходящий трафик на уровне неймспейса и компонентов, описанных в нём. Его поддержка должна быть реализована на уровне CNI-плагина.

## Что такое service mesh и для чего он нужен?  

Service mesh — это паттерн, позволяющий более гибко управлять трафиком в Kubernetes. Service mesh состоит из control plane, который собирает информацию о кластере k8s, запущенных в нём приложениях, а также дополнительных объектах (custom resources), которые могут быть описаны для дополнительной конфигурации, и так называемых sidecar-контейнерах, которые, как правило, инжектятся в PODы автоматически при помощи mutation webhook. Sidecar-контейнеры представляют собой reverse-proxy, перехватывающие входящий и исходящий трафик приложений в контейнерах и управляющие им в зависимости от полученной из control plane конфигурации.

Основные задачи, в которых используется service mesh, это:

- гарантия доставки трафика до приложений (service mesh реализует retry / timeouts / circuit breaker и другие подобные механизмы);
- увеличение безопасности при помощи шифрования трафика, верификация трафика на основе сертификатов, реализация дополнительных правил и разрешений на передачу трафика;
- tracing запросов, составление схемы и визуализация трафика;
- реализация переключения трафика при canary, a/b и blue/green стратегий деплоя.
## Какие есть недостатки у service mesh?  

Избыточная сложность, повышенная ресурсоёмкость и накладные расходы.
## В чем отличие меток (labels) от аннотаций (annotations)?

Метки используются для реализации механизмов поиска и группировки объектов, аннотации — для описания метаинформации на объекте (например, при помощи аннотаций мы можем запретить service mesh инжектировать sidecar-контейнеры в некоторые PODы).
## Из каких компонентов состоит k8s и каково их назначение?  

K8s состоит из control plane и data plane.  
  
**Control plane** — управляющий контур, который запускается в пределах нод, называемых master, и может запускаться как в одиночном режиме (single master), так и в распределённом (multi master).

Control plane содержит:
- ETCD — хранилище конфигурации кластера;
- Kubernetes API — предоставляет API, посредством которого взаимодействуют компоненты k8s, а также клиенты, находящиеся внутри и вне кластера;
- Kubernetes controller manager — реализует концепцию контроллеров, которые управляют базовыми сущностями кластеров (например, node controller, job controller, endpoint slice controller);
- Kubernetes scheduler — выбирает ноды, на которых нужно запускать PODы; 
- cloud controller manager — используется для реализации функций работы с облаком (если кластер k8s запущен в облаке).

**Data plane** — компоненты, которые запущены на каждой ноде:
- kubelet — следит за изменениями конфигурации ноды, применяет изменения конфигурации, делает пробы контейнеров, отчитывается о статусе контейнеров, работает с CRI-плагином и реализует функции запуска и остановки контейнеров;
- kube-proxy — отвечает за сетевой компонент: работает с CNI-плагином и обеспечивает функционирование сущности “сервис” (service) в пределах своей ноды.

## Что такое kube-proxy и для чего он нужен?

Это компонент Kubernetes data plane, который работает работает на каждой ноде. Он взаимодействует с CNI-плагином (обеспечивая функционирование pod network), и обеспечивает функционирование описанных в кластере сервисов (service) в пределах своей ноды, в зависимости от режима, выступая либо как прокси, либо как контроллер правил IPTABLES/IPVS.

## Что такое CRI?  

Аббревиатура расшифровывается как Container Runtime Interface. Это спецификация, описывающая некий уровень абстракции, который позволяет унифицированно использовать разные версии ПО для работы с контейнерами, например containerd или CRI-O.

## Допустим, у вас есть YAML-файл, в котором описаны deployment и service. Вы вводите команду kubectl create -f file.yaml . Как можно подробнее опишите, что произойдёт после нажатия клавиши Enter.

Прежде всего утилита kubectl найдёт и считает kubeconfig, возьмёт оттуда адрес Kubernetes API, сертификаты для его проверки, ключ доступа к нему и сделает запрос.  
Kubernetes API получит запрос, проверит права пользователя, которому принадлежит ключ, и, если права позволяют, запишет в ETCD информацию о новом deployment и service.  
Kubernetes controller manager увидит, что описан deployment, и при помощи Kubernetes API запишет в ETCD информацию о создании replicaset и нужного количества PODов на его основе. На этом этапе в работу могут включиться admission controller’ы, которые способны провалидировать корректность PODов или трансформировать их.  
Kubernetes scheduler увидит информацию о новых POD'ах и на основе их параметров (настроек требуемых ресурсов, nodeSelector'ов, taints/tolerations, podAntiAffinity и прочего) назначит PODы на подходящие ноды.  
Kubelet, в очередной раз опросив Kubernetes API, увидит изменение конфигурации ноды и начнёт вносить изменения: запустит контейнеры посредством CRI, даст команду kube-proxy на конфигурацию сети, запустит пробы, когда пробы будут пройдены, изменит в ETCD статус POD’а посредством Kubernetes API на Ready.  
Kubernetes controller manager увидит, что появились PODы в статусе Ready, которые соответствуют меткам, описанным в service, и создаст записи о новых Endpoint’ах для соответствующих PODов.  
Kubelet’ы на всех нодах кластера увидят изменение конфигурации и вызовут kube-proxy для создания Endpoint’ов.  
Kube-proxy вызовут CNI-плагины, которые реализуют Endpoint’ы на уровне инфраструктуры.  
При обращении к сервису трафик пойдёт на новые PODы.

## Можно ли запускать базы данных в k8s?  

При определенных условиях. Стоит помнить, что контейнеры не накладывают оверхеда по производительности приложений, но сетевой компонент в k8s, в зависимости от реализации, может накладывать определённые задержки. В контейнерах сложнее делать тонкий тюнинг приложений (менять настройки ядра ОС и прочие низкоуровневые настройки для повышения производительности), но это требуется нечасто. Так что если мы сможем обеспечить дисковое пространство с подходящими параметрами для хранения файлов БД и возможность их переноса между нодами (если такая возможность нужна), в k8s вполне можно запускать stateful-приложения как таковые. Более того, есть cloud ready / cloud native — базы и очереди, которые оптимизированы для запуска в k8s. Также для управления приложениями в k8s существуют операторы (Kubernetes operators), которые в том числе облегчают эксплуатацию stateful-приложений (например, postgresql stolon для управления кластерами postgresql, strimzi для управления kafka).

## В чем разница между deployment и statefulset?

В именах PODов: в deployment в качестве суффикса PODа используется некий случайный хеш, в statefulset — порядковый номер. К каждому PODу в statefulset можно обратиться по особым образом составленным доменным именам (это может быть полезно, например, для mongodb, где клиент должен знать имена всех нод кластера).

В способе работы с дисками (volume): statefulset обеспечивает алгоритм работы at most once — то есть дожидается, пока предыдущий POD с таким именем завершит работу, чтобы занять именно тот диск, который был к нему привязан, тогда как PODы deployment’а могут занять первый подходящий свободный.

В стратегиях перезапуска PODов при обновлении.

## Какие типы volum’ов можно использовать в k8s?

Можно использовать hostpath, чтобы подключить папку на ноде, но в этом случае POD должен быть привязан к ноде: если он переместится на другую ноду, использует там ту же папку, в которой не будет нужного содержимого.

Можно использовать local-storage — этот тип также использует папку на диске, но привязан к ноде и автоматически привязывает POD, который его использует, к нужной ноде.

Также мы можем использовать сетевые диски при помощи CSI-плагинов.

## Что такое CSI-плагин?  

Это абстракция, позволяющая унифицированно использовать сетевые файловые системы, построенные на разных технологических базах.

Мы описываем storageClass, соответствующий дискам определённого типа, и деплоим в кластер provisioner — специальное ПО, которое может заказывать сетевые диски в системе, способной их предоставлять (например, NAS или СХД).

Далее мы описываем объект persistentVolumeClaim с указанием нужного storageClass.

Provisioner при появлении PVC заказывает диск нужного размера в системе, которая их предоставляет, создаёт объект persistentVolume и привязывает его к PVC. Когда происходит запуск PODа на ноде, соответствующий диск монтируется на нужную ноду по определённому пути, и этот путь монтируется на файловую систему PODа.

## Каким образом мы можем разделять права в k8s?

Для разделения прав в Kubernetes применяется механизм RBAC (**Role Based Access Control**). В рамках него есть три группы сущностей — user, или service account, описывающий субъект доступа, role, или clusterRole, описывающий разрешения, и roleBinding, или clusterRoleBinding, для привязки списка разрешений к субъекту.

## Что такое role/clusterRole?

Роли описывают права при помощи наборов правил, содержащих:
- группы API — см. официальную документацию по apiGroups (https://Kubernetes.io/docs/reference/using-api/#api-groups) и вывод kubectl api-resources;
- ресурсы (resources: pod, namespace, deployment и т. п.);
- глаголы (verbs: set, update и т. п.);
- имена ресурсов (resourceNames) — для случая, когда нужно предоставить доступ к какому-то определённому ресурсу, а не ко всем ресурсам этого типа.

## Что такое service account?

Это специальные объекты в Kubernetes API (они так и называются — ServiceAccounts), которые привязаны к пространству имён и набору авторизационных данных, хранящихся в кластере в объектах типа Secrets. Такие пользователи (Service Accounts) предназначены в основном для управления правами доступа к Kubernetes API процессов, работающих в кластере Kubernetes.

## В чем отличие service account от user?

Users не имеют записей в Kubernetes API: управление ими должно осуществляться внешними механизмами. Они предназначены для людей или процессов, живущих вне кластера. Service Accounts существуют в рамках неймспейса k8s — их именование включает в себя имя неймспейса. Имена пользователей привязаны к кластеру и должны быть уникальны.

## Какие механизмы аутентификации используются в k8s?

Kubernetes может использовать большое количество механизмов аутентификации: сертификаты X509, Bearer-токены, аутентифицирующий прокси, HTTP Basic Auth. При помощи этих механизмов можно реализовать большое количество схем авторизации: от статичного файла с паролями до OpenID OAuth2.

Более того, допускается применение нескольких схем авторизации одновременно. По умолчанию в кластере используются:
- service account tokens — для Service Accounts;
- X509 — для Users.


## Что такое bearer token?

Bearer token, или service account token, — JWT-токен, который автоматически генерируется при создании нового service account (при этом он содержит служебную информацию о service account, которому выдан, и подписывается на корневом сертификате кластера), сохраняется в объект типа secret, который монтируется в POD по стандартному пути, и автоматически ротируется. Используя его, процесс, запущенный в контейнере, может обратиться к Kubernetes API и выполнить разрешенные действия.

## Что такое namespace в k8s и для чего он нужен?

Namespace в k8s реализует несколько функций. Прежде всего это способ группировки объектов, относящихся к одному приложению/проекту. Большинство объектов в k8s принадлежат неймспейсам (deployment, secret, service account), после помещения их в определённый неймспейс мы можем совместно их просматривать и применять к ним правила безопасности, квоты ресурсов, сетевые политики, правила service mesh. 

Кроме того, неймспейс участвует в формировании DNS-имён внутри кластера. Например, DNS-имена сервисов формируются по принципу [service name].[namespace].[суффикс кластера (по умолчанию svc.cluster.local)] 

## Что такое финалайзеры (finalizers) и для чего они нужны?

Это специальные ключи в манифесте объекта, описывающие действия, которые требуется совершить до удаления объекта. Например, они используются для того, чтобы невозможно было удалить pvc и pv при запущенном PODе.

## Допустим, у нас есть приложение, развёрнутое в k8s, которое доступно по адресу https://company.co . Обращаясь к этому адресу, вы видите задержку в ответе сайта и частые ошибки HTTP 504. Каковы ваши действия по отладке этой проблемы?

Прежде всего определю IP-адрес, куда ссылается https://company.co, и сущность, которая его поддерживает. Если это облачный балансер, посмотрю, куда он ведёт (скорее всего, это будут ноды кластера с сервисом типа nodePort/loadBalancer). Если это нода кластера, то, вероятно, это будет сервис с типом externalIp.

Следующим шагом найду этот сервис и посмотрю, куда он ссылается. Это может быть ingress-контроллер либо непосредственно само приложение.

Определю нэймспейс, в котором развёрнуто приложение / описан ингресс, и прежде всего посмотрю на состояние его PODов — статусы, количество рестартов, время жизни, быстрые метрики, если есть (kubectl top pods).

Если и на этом этапе не обнаружу проблем, взгляну также на статус и метрики PODов ингресс-контроллера.

Если время жизни PODов небольшое, взгляну на статус нод в кластере (все ли Ready), события в кластере и неймспейсе (kubectl get events), наличие PODов в статусе evicted (кончились ресурсы на какой-то из нод) и быстрые метрики нод (kubectl top nodes). А также проверю проект приложения в CI/CD-системе (или инфраструктурном репозитории, если у нас gitops) на наличие выкатов в обозримом прошлом.

Если были выкаты приложения, посмотрю по истории git, кто и какие изменения делал, могли ли они повлиять на работоспособность, привлеку к решению проблемы разработчиков и, возможно, откачу выкат.

Если есть PODы в нерабочем статусе либо с большим количеством перезагрузок, посмотрю по логам PODов, что может мешать нормальной работе.

Если время жизни PODов большое или быстрые метрики показывают проблемы, буду искать проблемы при помощи системы мониторинга — просмотрю базовые показатели производительности нод/PODов, утилизацию ресурсов в разрезе установленных лимитов, данные об ошибках / количестве запросов / времени ответа приложения, полученных на ингресс-контроллере (если он есть), утилизацию сети PODами, состояние сети на нодах. Если есть service mesh — проверю её состояние, статус control plane, утилизацию ресурсов sidecar-контейнерами, зайду в интерфейс администратора service mesh и просмотрю метрики о работе приложения, которые она предоставляет.

Проверю по мониторингу состояние смежных сервисов, которые использует приложение (базы данных, очереди, сервисы).  
Если вышеперечисленные действия не позволили обнаружить причину инцидента, начну пристальное изучение логов приложения.

Наверняка на каком-то из этих этапов обнаружится проблема, расследуя которую, можно наткнуться на корневую причину инцидента.
